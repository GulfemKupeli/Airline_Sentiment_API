# -*- coding: utf-8 -*-
"""TwitterSentimentAnalysisModelTraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l9nd-JdmQVZn93Gkm8dheb3X_ghB4XaL
"""

from google.colab import drive
import pandas as pd
import numpy as np
import re
import time

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer


nltk.download('stopwords')
nltk.download('wordnet')

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Colab Notebooks/Airline_Sentiment/Tweets.csv'

try:
  df = pd.read_csv(file_path)

  print(f"Dataset loaded successfully from Google Drive with {df.shape[0]} rows and {df.shape[1]} columns.")
  if 'airline_sentiment' not in df.columns:
        df = df.rename(columns={'airline_sentiment_level': 'airline_sentiment'})

  stop_words = set(stopwords.words('english'))
  lemmatizer = WordNetLemmatizer()

  def preprocessing_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()

    tokens = text.split()
    cleaned_tokens = [
          lemmatizer.lemmatize(word) for word in tokens if word not in stop_words
      ]
    return " ".join(cleaned_tokens)

  df['processed_text'] = df['text'].astype(str).apply(preprocessing_text)

  X = df['processed_text']
  Y = df['airline_sentiment']

  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify = Y )

  tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
  X_train_vec = tfidf_vectorizer.fit_transform(X_train)
  X_test_vec = tfidf_vectorizer.transform(X_test)

  print(f"\nTraining set vector shape: {X_train_vec.shape}")
  print(f"Test set vector shape: {X_test_vec.shape}")

except FileNotFoundError:
    print("Error: File not found at the specified Google Drive path. Please check the 'file_path' variable.")
except NameError:
    # This should not happen with the fix, but good practice to catch potential issues
    print("Error: 'df' is not defined. This might indicate an issue with data loading.")

model = LogisticRegression(solver='saga', max_iter=1000, random_state=42, n_jobs=-1)

print("Training the model...")
model.fit(X_train_vec, Y_train)

print("Train complete.")
Y_pred = model.predict(X_test_vec)

print("\n--- Model Review ---")
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report :")
print(classification_report(Y_test, Y_pred))

cm = confusion_matrix(Y_test, Y_pred, labels=model.classes_)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix (Logistic Regression)')
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

model_weighted = LogisticRegression(solver = 'saga', max_iter=1000, random_state=42, n_jobs=-1, class_weight='balanced')

print('Training the weighet logistic regression model...')
model_weighted.fit(X_train_vec, Y_train)

print("Train complete.")
Y_pred_weighted = model_weighted.predict(X_test_vec)

print("\n--- Weighted Model Review ---")
accuracy_weighted = accuracy_score(Y_test, Y_pred_weighted)
print(f"Accuracy: {accuracy_weighted:.4f}")

print("\nNew Classification Report :")
print(classification_report(Y_test, Y_pred_weighted))
cm_weighted = confusion_matrix(Y_test, Y_pred_weighted, labels=model_weighted.classes_)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues',
            xticklabels=model_weighted.classes_, yticklabels=model_weighted.classes_)
plt.title('Confusion Matrix (Weighted Logistic Regression)')
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

plt.figure(figsize=(7, 5))
sns.countplot(x='airline_sentiment', data=df,
              order=df['airline_sentiment'].value_counts().index,
              palette=['#d9534f', '#5cb85c', '#f0ad4e'])
plt.title('Airline Sentiment Distribution', fontsize=15)
plt.xlabel('Sentiment Class', fontsize=12)
plt.ylabel('Tweet Count', fontsize =12)
plt.show()

all_words = ' '.join([text for text in df['processed_text']])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100,background_color='white').generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Stop Words Substituted', fontsize=15)
plt.show()

negative_tweets = df[df['airline_sentiment'] == 'negative']['processed_text']
negative_words = ' '.join([text for text in negative_tweets])

wordcloud_neg = WordCloud(width=800, height=500, random_state=42, max_font_size=100,
                          background_color='red', colormap='Reds').generate(negative_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_neg, interpolation="bilinear")
plt.axis('off')
plt.title('Most frequent words used in tweets', fontsize=15)
plt.show()

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import Dataset, Features, Value, ClassLabel
from transformers import Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

print(f"Is CUDA available? {torch.cuda.is_available()}")

df_bert = df[['processed_text', 'airline_sentiment']].copy()

sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
df_bert['labels'] = df_bert['airline_sentiment'].map(sentiment_mapping)

print("Dataset is ready for BERT.")
print(df_bert.head())

import torch
import pandas as pd
from datasets import Dataset, Features, Value, ClassLabel

df_bert = df[['text', 'airline_sentiment']].copy()

sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
df_bert['labels'] = df_bert['airline_sentiment'].map(sentiment_mapping)


print(f"CUDA Mevcut mu: {torch.cuda.is_available()}")
print("BERT için veri seti hazırlandı (Orijinal Ham Metin kullanılıyor).")
print(df_bert.head())

features = Features({
    'text': Value('string'),
    'airline_sentiment': Value('string'),
    'labels': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])
})
dataset = Dataset.from_pandas(df_bert.reset_index(drop=True), features=features)
dataset = dataset.train_test_split(test_size=0.2, seed=42)

model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    f1_macro = f1_score(labels, predictions, average="macro")
    accuracy = accuracy_score(labels, predictions)

    return {"accuracy": accuracy, "f1_macro": f1_macro}

def tokenize_function(examples):
  return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["test"]

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)


training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    eval_strategy="epoch",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\n--- BERT Model İnce Ayar Eğitimi Başlıyor (3 Epoch) ---")
trainer.train()

print("\n--- BERT Modelinin Sonuçları ---")
results = trainer.evaluate()
print(results)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# --- 1. BERT Modelinden Tahminleri Tekrar Al ---
# Trainer, Y_true_bert'i içeren 'predictions' nesnesini döndürür.
predictions = trainer.predict(eval_dataset)
logits = predictions.predictions
Y_pred_bert = np.argmax(logits, axis=-1)
Y_true_bert = predictions.label_ids # İşte aradığımız değişken!

# Orijinal etiket isimlerini tanımlama (Sözlükten tersine çevirme)
label_names = ['negative', 'neutral', 'positive']

# 2. Sayısal Etiketleri Geri Metne Çevirme (Görselleştirme için)
Y_pred_labels = [label_names[p] for p in Y_pred_bert]
Y_true_labels = [label_names[l] for l in Y_true_bert]

# --- 3. Nihai Sınıflandırma Raporu ---
print("\n--- BERT Modelinin Sınıflandırma Raporu (Final) ---")
print(classification_report(Y_true_labels, Y_pred_labels, target_names=label_names))

# --- 4. Karışıklık Matrisi Görselleştirmesi ---
cm_bert = confusion_matrix(Y_true_labels, Y_pred_labels, labels=label_names)

plt.figure(figsize=(9, 7))
sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_names, yticklabels=label_names,
            linecolor='black', linewidths=.5)

plt.title(f'BERT Karışıklık Matrisi (Accuracy: {accuracy_score(Y_true_labels, Y_pred_labels):.4f})', fontsize=16)
plt.ylabel('Gerçek Sınıf', fontsize=12)
plt.xlabel('Tahmin Edilen Sınıf', fontsize=12)
plt.show()

# --- 5. Model Karşılaştırma Özeti (Rapor için) ---
print("\n--- Model Karşılaştırma Özeti (F1-Score) ---")
print(f"Lojistik Regresyon (Ağırlıklandırılmış) Macro F1: 0.71")
print(f"BERT (İnce Ayarlı) Macro F1: {f1_score(Y_true_labels, Y_pred_labels, average='macro'):.4f}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

# Modeli Google Drive'a Kaydetme
# Lütfen aşağıdaki yolu kendi Google Drive'ınızdaki istediğiniz klasör yolu ile değiştirin.
save_directory = "/content/drive/MyDrive/Colab Notebooks/Airline_Sentiment/airline_bert_model"

# Eğer klasör yoksa oluştur
import os
if not os.path.exists(save_directory):
    os.makedirs(save_directory)

# Modeli ve Tokenizer'ı Kaydet
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f"Model ve tokenizer başarıyla kaydedildi: {save_directory}")

import shutil
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

# --- 1. Yolları Tanımlama ---
drive_model_path = "/content/drive/MyDrive/Colab Notebooks/Airline_Sentiment/airline_bert_model"
local_model_path = "./local_bert_model"
label_names = ['negative', 'neutral', 'positive']

# --- 2. Kopyalama İşlemi (Drive Hatalarından Kaçınmak İçin) ---
print(f"Drive'dan yerel klasöre kopyalanıyor: {drive_model_path}")
try:
    if os.path.exists(local_model_path):
        shutil.rmtree(local_model_path)

    shutil.copytree(drive_model_path, local_model_path)
    print(f"Kopyalama Başarılı! Model artık yerel dizinde: {local_model_path}")

except Exception as e:
    print(f"KRİTİK HATA: Kopyalama başarısız oldu. Hata: {e}")

# --- 3. Model Yükleme Fonksiyonu ---
global loaded_model, loaded_tokenizer, device
loaded_model = None
loaded_tokenizer = None
device = None

def load_bert_model(model_path):
    global loaded_model, loaded_tokenizer, device
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)
        loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)
        loaded_model.to(device)
        print(f"\nModel {model_path} yolundan BAŞARILIYLA yüklendi.")
        return True

    except Exception as e:
        print(f"KRİTİK HATA: Model yüklenirken bir sorun oluştu. Hata: {e}")
        return False

# Modeli YÜKLE
load_bert_model(local_model_path)

def predict_sentiment(text_input):
    if loaded_model is None:
        return "HATA: Model yüklenmediği için tahmin yapılamıyor."

    inputs = loaded_tokenizer(text_input, return_tensors="pt", padding=True, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = loaded_model(**inputs)

    logits = outputs.logits
    prediction_index = torch.argmax(logits, dim=-1).item()

    return label_names[prediction_index]

# --- 4. SON TEST VE GİTHUB ÇIKTISI ---
print("\n--- Model Testi ---")
test_tweet_neg = "My flight was canceled and I spent six hours waiting at the gate. Terrible service."
test_tweet_pos = "Great service and the staff was super friendly! Best flight experience ever."

print(f"Negatif Test Tahmini: {predict_sentiment(test_tweet_neg)}")
print(f"Pozitif Test Tahmini: {predict_sentiment(test_tweet_pos)}")

# Eğer bu testler başarılı olursa:
print("\nPROJE BAŞARILI! Bu fonksiyonlar GitHub'daki API'nizin temelini oluşturacaktır.")